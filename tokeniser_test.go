package bash

import (
	"testing"

	"vimagination.zapto.org/parser"
)

func TestTokeniser(t *testing.T) {
	for n, test := range [...]struct {
		Input  string
		Output []parser.Token
	}{
		{ // 1
			"",
			[]parser.Token{
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 2
			" ",
			[]parser.Token{
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 3
			" \t\\\n",
			[]parser.Token{
				{Type: TokenWhitespace, Data: " \t\\\n"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 4
			"\\\n \t",
			[]parser.Token{
				{Type: TokenWhitespace, Data: "\\\n \t"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 5
			" \n\n \n",
			[]parser.Token{
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenLineTerminator, Data: "\n\n"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 6
			"#A comment\n# B comment",
			[]parser.Token{
				{Type: TokenComment, Data: "#A comment"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenComment, Data: "# B comment"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 7
			"$ident $name a\\nbc=a $0 $12 a$b a${b}c $$ $! $?",
			[]parser.Token{
				{Type: TokenIdentifier, Data: "$ident"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifier, Data: "$name"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a\\nbc=a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifier, Data: "$0"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifier, Data: "$1"},
				{Type: TokenWord, Data: "2"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenIdentifier, Data: "$b"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "b"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWord, Data: "c"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifier, Data: "$$"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifier, Data: "$!"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifier, Data: "$?"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 8
			"abc=a def[0]=b ghi[$i]=c jkl+=d",
			[]parser.Token{
				{Type: TokenIdentifierAssign, Data: "abc"},
				{Type: TokenPunctuator, Data: "="},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifierAssign, Data: "def"},
				{Type: TokenPunctuator, Data: "["},
				{Type: TokenWord, Data: "0"},
				{Type: TokenPunctuator, Data: "]"},
				{Type: TokenPunctuator, Data: "="},
				{Type: TokenWord, Data: "b"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifierAssign, Data: "ghi"},
				{Type: TokenPunctuator, Data: "["},
				{Type: TokenIdentifier, Data: "$i"},
				{Type: TokenPunctuator, Data: "]"},
				{Type: TokenPunctuator, Data: "="},
				{Type: TokenWord, Data: "c"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifierAssign, Data: "jkl"},
				{Type: TokenPunctuator, Data: "+="},
				{Type: TokenWord, Data: "d"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 9
			"ident ${name} ab\\nc=a ${6} a$ ",
			[]parser.Token{
				{Type: TokenWord, Data: "ident"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "name"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "ab\\nc=a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenNumberLiteral, Data: "6"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a$"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 10
			"$(( 0 1 29 0xff 0xDeAdBeEf 0755 2#5 ))",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "$(("},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenNumberLiteral, Data: "0"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenNumberLiteral, Data: "1"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenNumberLiteral, Data: "29"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenNumberLiteral, Data: "0xff"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenNumberLiteral, Data: "0xDeAdBeEf"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenNumberLiteral, Data: "0755"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenNumberLiteral, Data: "2#5"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "))"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 11
			"\"abc\" \"de\\nf\" \"stuff`command`more stuff\" \"text $ident $another end\" \"text $(command) end - text ${ident} end\" \"with\nnewline\" 'with\nnewline' $\"a string\" $'a \\'string'",
			[]parser.Token{
				{Type: TokenString, Data: "\"abc\""},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenString, Data: "\"de\\nf\""},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenStringStart, Data: "\"stuff"},
				{Type: TokenOpenBacktick, Data: "`"},
				{Type: TokenWord, Data: "command"},
				{Type: TokenCloseBacktick, Data: "`"},
				{Type: TokenStringEnd, Data: "more stuff\""},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenStringStart, Data: "\"text "},
				{Type: TokenIdentifier, Data: "$ident"},
				{Type: TokenStringMid, Data: " "},
				{Type: TokenIdentifier, Data: "$another"},
				{Type: TokenStringEnd, Data: " end\""},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenStringStart, Data: "\"text "},
				{Type: TokenPunctuator, Data: "$("},
				{Type: TokenWord, Data: "command"},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenStringMid, Data: " end - text "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "ident"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenStringEnd, Data: " end\""},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenString, Data: "\"with\nnewline\""},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenString, Data: "'with\nnewline'"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenString, Data: "$\"a string\""},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenString, Data: "$'a \\'string'"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 12
			"< <<< <& <> > >> >& &>> >| | |& || & && () {} + = += `` $() $(()) (())",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "<<<"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "<&"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "<>"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: ">"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: ">>"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: ">&"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "&>>"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: ">|"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "|"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "|&"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "||"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "&"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "&&"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "("},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "{"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "+"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "+="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenOpenBacktick, Data: "`"},
				{Type: TokenCloseBacktick, Data: "`"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "$("},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "$(("},
				{Type: TokenPunctuator, Data: "))"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "(("},
				{Type: TokenPunctuator, Data: "))"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 13
			"$(( + += - -= & &= | |= < <= > >= = == ! != * *= ** / /= % %= ^ ^= ~ ? : , (1) ))",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "$(("},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "+"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "+="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "-"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "-="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "&"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "&="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "|"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "|="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "<"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "<="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: ">"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: ">="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "=="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "!"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "!="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "*"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "*="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "**"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "/"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "/="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "%"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "%="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "^"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "^="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "~"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "?"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: ":"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: ","},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "("},
				{Type: TokenNumberLiteral, Data: "1"},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "))"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 14
			"$(( a+b 1+2))",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "$(("},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenPunctuator, Data: "+"},
				{Type: TokenWord, Data: "b"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenNumberLiteral, Data: "1"},
				{Type: TokenPunctuator, Data: "+"},
				{Type: TokenNumberLiteral, Data: "2"},
				{Type: TokenPunctuator, Data: "))"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 15
			"(( a+b 1+2))",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "(("},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenPunctuator, Data: "+"},
				{Type: TokenWord, Data: "b"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenNumberLiteral, Data: "1"},
				{Type: TokenPunctuator, Data: "+"},
				{Type: TokenNumberLiteral, Data: "2"},
				{Type: TokenPunctuator, Data: "))"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 16
			"$(( ( ))",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "$(("},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "("},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: ")"},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 17
			"$(( ? ))",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "$(("},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "?"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 18
			"{ )",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "{"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 19
			"(",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "("},
				{Type: parser.TokenError, Data: "unexpected EOF"},
			},
		},
		{ // 20
			"$(",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "$("},
				{Type: parser.TokenError, Data: "unexpected EOF"},
			},
		},
		{ // 21
			"$(}",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "$("},
				{Type: TokenPunctuator, Data: "}"},
				{Type: parser.TokenError, Data: "unexpected EOF"},
			},
		},
		{ // 22
			"<<abc\n123\n456\nabc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "abc"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredoc, Data: "123\n456\n"},
				{Type: TokenHeredocEnd, Data: "abc"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 23
			"<< abc\n123\n456\nabc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "abc"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredoc, Data: "123\n456\n"},
				{Type: TokenHeredocEnd, Data: "abc"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 24
			"<<-abc\n123\n456\nabc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<-"},
				{Type: TokenWord, Data: "abc"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredoc, Data: "123\n456\n"},
				{Type: TokenHeredocEnd, Data: "abc"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 25
			"<<-abc\n\t123\n\t\t456\n\t\t\tabc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<-"},
				{Type: TokenWord, Data: "abc"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredocIndent, Data: "\t"},
				{Type: TokenHeredoc, Data: "123\n"},
				{Type: TokenHeredocIndent, Data: "\t\t"},
				{Type: TokenHeredoc, Data: "456\n"},
				{Type: TokenHeredocIndent, Data: "\t\t\t"},
				{Type: TokenHeredocEnd, Data: "abc"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 26
			"<<a'b 'c\n123\n456\nab c\n",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "a'b 'c"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredoc, Data: "123\n456\n"},
				{Type: TokenHeredocEnd, Data: "ab c"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 27
			"<<def\n123\n456\ndef\nabc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "def"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredoc, Data: "123\n456\n"},
				{Type: TokenHeredocEnd, Data: "def"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenWord, Data: "abc"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 28
			"<<def cat\n123\n456\ndef\nabc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "def"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "cat"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredoc, Data: "123\n456\n"},
				{Type: TokenHeredocEnd, Data: "def"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenWord, Data: "abc"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 29
			"<<abc cat;<<def cat\n123\nabc\n456\ndef",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "abc"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "cat"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "def"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "cat"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredoc, Data: "123\n"},
				{Type: TokenHeredocEnd, Data: "abc"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredoc, Data: "456\n"},
				{Type: TokenHeredocEnd, Data: "def"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 30
			"<<abc cat;echo $(<<def cat\n456\ndef\n)\n123\nabc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "abc"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "cat"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWord, Data: "echo"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "$("},
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "def"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "cat"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredoc, Data: "456\n"},
				{Type: TokenHeredocEnd, Data: "def"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredoc, Data: "123\n"},
				{Type: TokenHeredocEnd, Data: "abc"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 31
			"<<abc\na$abc\nabc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "abc"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredoc, Data: "a"},
				{Type: TokenIdentifier, Data: "$abc"},
				{Type: TokenHeredoc, Data: "\n"},
				{Type: TokenHeredocEnd, Data: "abc"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 32
			"<<'abc'\na$abc\nabc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "'abc'"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredoc, Data: "a$abc\n"},
				{Type: TokenHeredocEnd, Data: "abc"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 33
			"<<\"\"abc\na$abc\nabc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "\"\"abc"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredoc, Data: "a$abc\n"},
				{Type: TokenHeredocEnd, Data: "abc"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 34
			"<<a\\ b\\ c\na$abc\na b c",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "a\\ b\\ c"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredoc, Data: "a$abc\n"},
				{Type: TokenHeredocEnd, Data: "a b c"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 35
			"<<abc\na${abc} $99\nabc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "abc"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredoc, Data: "a"},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "abc"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenHeredoc, Data: " "},
				{Type: TokenIdentifier, Data: "$9"},
				{Type: TokenHeredoc, Data: "9\n"},
				{Type: TokenHeredocEnd, Data: "abc"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 36
			"<<abc\na$(\necho abc;\n) 1\nabc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "abc"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredoc, Data: "a"},
				{Type: TokenPunctuator, Data: "$("},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenWord, Data: "echo"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "abc"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenHeredoc, Data: " 1\n"},
				{Type: TokenHeredocEnd, Data: "abc"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 37
			"<<abc\n$a\nabc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "abc"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenIdentifier, Data: "$a"},
				{Type: TokenHeredoc, Data: "\n"},
				{Type: TokenHeredocEnd, Data: "abc"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 38
			"<<abc\n$$\nabc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "abc"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenIdentifier, Data: "$$"},
				{Type: TokenHeredoc, Data: "\n"},
				{Type: TokenHeredocEnd, Data: "abc"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 39
			"<<abc\n$!\nabc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "abc"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenIdentifier, Data: "$!"},
				{Type: TokenHeredoc, Data: "\n"},
				{Type: TokenHeredocEnd, Data: "abc"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 40
			"<<abc\n$?\nabc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "abc"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenIdentifier, Data: "$?"},
				{Type: TokenHeredoc, Data: "\n"},
				{Type: TokenHeredocEnd, Data: "abc"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 41
			"<<abc\nabc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "abc"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredocEnd, Data: "abc"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 42
			"<<abc\na$(<<def) 1\nabc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "abc"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredoc, Data: "a"},
				{Type: TokenPunctuator, Data: "$("},
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "def"},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 43
			"<<abc\na$(<<def cat) 1\nabc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "abc"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredoc, Data: "a"},
				{Type: TokenPunctuator, Data: "$("},
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "def"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "cat"},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 44
			"<<abc;$(<<def cat)\nabc\ndef\nabc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "abc"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenPunctuator, Data: "$("},
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "def"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "cat"},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 45
			"<<abc;<<def;$(<<ghi;<<jkl\nghi\njkl\n)\nabc\ndef",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "abc"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "def"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenPunctuator, Data: "$("},
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "ghi"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "jkl"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredocEnd, Data: "ghi"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredocEnd, Data: "jkl"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredocEnd, Data: "abc"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredocEnd, Data: "def"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 46
			"<<a\\\nbc\nabc\ndef\na\nbc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "a\\\nbc"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredoc, Data: "abc\ndef\n"},
				{Type: TokenHeredocEnd, Data: "a\nbc"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 47
			"<<a;echo ${a/b/\n$c #not-a-comment $d}\n123\na",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "a"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWord, Data: "echo"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "/"},
				{Type: TokenPattern, Data: "b"},
				{Type: TokenPunctuator, Data: "/"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenIdentifier, Data: "$c"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "#not-a-comment"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifier, Data: "$d"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredoc, Data: "123\n"},
				{Type: TokenHeredocEnd, Data: "a"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 48
			"2>1 word",
			[]parser.Token{
				{Type: TokenNumberLiteral, Data: "2"},
				{Type: TokenPunctuator, Data: ">"},
				{Type: TokenWord, Data: "1"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "word"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 49
			"time -p cmd",
			[]parser.Token{
				{Type: TokenKeyword, Data: "time"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "-p"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "cmd"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 50
			"time -p cmd if",
			[]parser.Token{
				{Type: TokenKeyword, Data: "time"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "-p"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "cmd"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "if"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 51
			"time -p if a;then b;fi",
			[]parser.Token{
				{Type: TokenKeyword, Data: "time"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "-p"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "if"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenKeyword, Data: "then"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenKeyword, Data: "fi"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 52
			"{a..b..2} {a,b,d} a{b,c,d}e a{1..4} {2..10..-1} {-1..-100..5} {a..z..-1}",
			[]parser.Token{
				{Type: TokenBraceExpansion, Data: "{a..b..2}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenBraceExpansion, Data: "{a,b,d}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenBraceExpansion, Data: "{b,c,d}"},
				{Type: TokenWord, Data: "e"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenBraceExpansion, Data: "{1..4}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenBraceExpansion, Data: "{2..10..-1}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenBraceExpansion, Data: "{-1..-100..5}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenBraceExpansion, Data: "{a..z..-1}"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 53
			"a={123",
			[]parser.Token{
				{Type: TokenIdentifierAssign, Data: "a"},
				{Type: TokenPunctuator, Data: "="},
				{Type: TokenWord, Data: "{123"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 54
			"word{ word{a} word{\nword{",
			[]parser.Token{
				{Type: TokenWord, Data: "word{"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "word{a}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "word{"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenWord, Data: "word{"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 55
			"{ echo 123; echo 456; }",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "{"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "echo"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "123"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "echo"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "456"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "}"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 56
			"(echo 123; echo 456)",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "("},
				{Type: TokenWord, Data: "echo"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "123"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "echo"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "456"},
				{Type: TokenPunctuator, Data: ")"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 57
			"`a` `echo \\`abc\\`` echo \"a`echo \"1\\`echo u\\\\\\`echo 123\\\\\\`v\\`3\"`c\"",
			[]parser.Token{
				{Type: TokenOpenBacktick, Data: "`"},
				{Type: TokenWord, Data: "a"},
				{Type: TokenCloseBacktick, Data: "`"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenOpenBacktick, Data: "`"},
				{Type: TokenWord, Data: "echo"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenOpenBacktick, Data: "\\`"},
				{Type: TokenWord, Data: "abc"},
				{Type: TokenCloseBacktick, Data: "\\`"},
				{Type: TokenCloseBacktick, Data: "`"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "echo"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenStringStart, Data: "\"a"},
				{Type: TokenOpenBacktick, Data: "`"},
				{Type: TokenWord, Data: "echo"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenStringStart, Data: "\"1"},
				{Type: TokenOpenBacktick, Data: "\\`"},
				{Type: TokenWord, Data: "echo"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "u"},
				{Type: TokenOpenBacktick, Data: "\\\\\\`"},
				{Type: TokenWord, Data: "echo"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "123"},
				{Type: TokenCloseBacktick, Data: "\\\\\\`"},
				{Type: TokenWord, Data: "v"},
				{Type: TokenCloseBacktick, Data: "\\`"},
				{Type: TokenStringEnd, Data: "3\""},
				{Type: TokenCloseBacktick, Data: "`"},
				{Type: TokenStringEnd, Data: "c\""},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 58
			"`\\``",
			[]parser.Token{
				{Type: TokenOpenBacktick, Data: "`"},
				{Type: TokenOpenBacktick, Data: "\\`"},
				{Type: parser.TokenError, Data: "incorrect backtick depth"},
			},
		},
		{ // 59
			"`\\`\\\\\\``",
			[]parser.Token{
				{Type: TokenOpenBacktick, Data: "`"},
				{Type: TokenOpenBacktick, Data: "\\`"},
				{Type: TokenOpenBacktick, Data: "\\\\\\`"},
				{Type: parser.TokenError, Data: "incorrect backtick depth"},
			},
		},
		{ // 60
			"`\\`\\\\\\`\\`",
			[]parser.Token{
				{Type: TokenOpenBacktick, Data: "`"},
				{Type: TokenOpenBacktick, Data: "\\`"},
				{Type: TokenOpenBacktick, Data: "\\\\\\`"},
				{Type: parser.TokenError, Data: "incorrect backtick depth"},
			},
		},
		{ // 61
			"`\\$abc`",
			[]parser.Token{
				{Type: TokenOpenBacktick, Data: "`"},
				{Type: TokenIdentifier, Data: "\\$abc"},
				{Type: TokenCloseBacktick, Data: "`"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 62
			"echo `echo \\\"abc\\\"`",
			[]parser.Token{
				{Type: TokenWord, Data: "echo"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenOpenBacktick, Data: "`"},
				{Type: TokenWord, Data: "echo"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "\\\"abc\\\""},
				{Type: TokenCloseBacktick, Data: "`"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 63
			"\\\"abc\\\"",
			[]parser.Token{
				{Type: TokenWord, Data: "\\\"abc\\\""},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 64
			"\\",
			[]parser.Token{
				{Type: parser.TokenError, Data: "unexpected EOF"},
			},
		},
		{ // 65
			"{abc}>2",
			[]parser.Token{
				{Type: TokenBraceWord, Data: "{abc}"},
				{Type: TokenPunctuator, Data: ">"},
				{Type: TokenWord, Data: "2"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 66
			"<&1-",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<&"},
				{Type: TokenWord, Data: "1-"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 67
			": ${!a} ${!a*} ${!a@} ${!a[@]} ${!a[*]} ${a:1:2} ${a: -1 : -2} ${a:1} ${a:-b} ${a:=b} ${a:?a is unset} ${a:+a is set} ${#a} ${#} ${a#b} ${a##b} ${a%b} ${a%%b} ${a/b/c} ${a//b/c} ${a/#b/c} ${a/%b/c} ${a^b} ${a^^b} ${a,b} ${a,,b} ${a@Q} ${a@a} ${a@P}",
			[]parser.Token{
				{Type: TokenWord, Data: ":"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenPunctuator, Data: "!"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenPunctuator, Data: "!"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "*"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenPunctuator, Data: "!"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "@"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenPunctuator, Data: "!"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "["},
				{Type: TokenWord, Data: "@"},
				{Type: TokenPunctuator, Data: "]"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenPunctuator, Data: "!"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "["},
				{Type: TokenWord, Data: "*"},
				{Type: TokenPunctuator, Data: "]"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: ":"},
				{Type: TokenNumberLiteral, Data: "1"},
				{Type: TokenPunctuator, Data: ":"},
				{Type: TokenNumberLiteral, Data: "2"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: ":"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenNumberLiteral, Data: "-1"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: ":"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenNumberLiteral, Data: "-2"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: ":"},
				{Type: TokenNumberLiteral, Data: "1"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: ":-"},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: ":="},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: ":?"},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "is"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "unset"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: ":+"},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "is"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "set"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenPunctuator, Data: "#"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenKeyword, Data: "#"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "#"},
				{Type: TokenPattern, Data: "b"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "##"},
				{Type: TokenPattern, Data: "b"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "%"},
				{Type: TokenPattern, Data: "b"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "%%"},
				{Type: TokenPattern, Data: "b"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "/"},
				{Type: TokenPattern, Data: "b"},
				{Type: TokenPunctuator, Data: "/"},
				{Type: TokenWord, Data: "c"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "//"},
				{Type: TokenPattern, Data: "b"},
				{Type: TokenPunctuator, Data: "/"},
				{Type: TokenWord, Data: "c"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "/#"},
				{Type: TokenPattern, Data: "b"},
				{Type: TokenPunctuator, Data: "/"},
				{Type: TokenWord, Data: "c"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "/%"},
				{Type: TokenPattern, Data: "b"},
				{Type: TokenPunctuator, Data: "/"},
				{Type: TokenWord, Data: "c"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "^"},
				{Type: TokenPattern, Data: "b"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "^^"},
				{Type: TokenPattern, Data: "b"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: ","},
				{Type: TokenPattern, Data: "b"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: ",,"},
				{Type: TokenPattern, Data: "b"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "@"},
				{Type: TokenBraceWord, Data: "Q"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "@"},
				{Type: TokenBraceWord, Data: "a"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "@"},
				{Type: TokenBraceWord, Data: "P"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 68
			"${a/[/c}",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "/"},
				{Type: parser.TokenError, Data: "unexpected EOF"},
			},
		},
		{ // 69
			"${a/\\[/c}",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "/"},
				{Type: TokenPattern, Data: "\\["},
				{Type: TokenPunctuator, Data: "/"},
				{Type: TokenWord, Data: "c"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 70
			"${a/[b]/c}",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "/"},
				{Type: TokenPattern, Data: "[b]"},
				{Type: TokenPunctuator, Data: "/"},
				{Type: TokenWord, Data: "c"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 71
			"${a/(/c}",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "/"},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 72
			"${a/\\(/c}",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "/"},
				{Type: TokenPattern, Data: "\\("},
				{Type: TokenPunctuator, Data: "/"},
				{Type: TokenWord, Data: "c"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 73
			"${a/(b)/c}",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "/"},
				{Type: TokenPattern, Data: "(b)"},
				{Type: TokenPunctuator, Data: "/"},
				{Type: TokenWord, Data: "c"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 74
			"${a@Z}",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "@"},
				{Type: parser.TokenError, Data: "invalid parameter expansion"},
			},
		},
		{ // 75
			"${@} ${*}",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenKeyword, Data: "@"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenKeyword, Data: "*"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 76
			"$() $(()) `` ${}",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "$("},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "$(("},
				{Type: TokenPunctuator, Data: "))"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenOpenBacktick, Data: "`"},
				{Type: TokenCloseBacktick, Data: "`"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "${"},
				{Type: parser.TokenError, Data: "invalid parameter expansion"},
			},
		},
		{ // 77
			"case a in b)c;;esac",
			[]parser.Token{
				{Type: TokenKeyword, Data: "case"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "in"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenWord, Data: "c"},
				{Type: TokenPunctuator, Data: ";;"},
				{Type: TokenKeyword, Data: "esac"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 78
			"case a in b;;esac",
			[]parser.Token{
				{Type: TokenKeyword, Data: "case"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "in"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 79
			"case a in esac",
			[]parser.Token{
				{Type: TokenKeyword, Data: "case"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "in"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "esac"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 80
			"case a in b)c;;d)e;&f)g;;&h)i\nesac",
			[]parser.Token{
				{Type: TokenKeyword, Data: "case"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "in"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenWord, Data: "c"},
				{Type: TokenPunctuator, Data: ";;"},
				{Type: TokenWord, Data: "d"},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenWord, Data: "e"},
				{Type: TokenPunctuator, Data: ";&"},
				{Type: TokenWord, Data: "f"},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenWord, Data: "g"},
				{Type: TokenPunctuator, Data: ";;&"},
				{Type: TokenWord, Data: "h"},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenWord, Data: "i"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenKeyword, Data: "esac"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 81
			"case a b)c;;esac",
			[]parser.Token{
				{Type: TokenKeyword, Data: "case"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "missing in"},
			},
		},
		{ // 82
			"case a in b)c;;",
			[]parser.Token{
				{Type: TokenKeyword, Data: "case"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "in"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenWord, Data: "c"},
				{Type: TokenPunctuator, Data: ";;"},
				{Type: parser.TokenError, Data: "unexpected EOF"},
			},
		},
		{ // 83
			"if a; then b; fi",
			[]parser.Token{
				{Type: TokenKeyword, Data: "if"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "then"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "fi"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 84
			"if a;\nthen\nb\nfi",
			[]parser.Token{
				{Type: TokenKeyword, Data: "if"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenKeyword, Data: "then"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenWord, Data: "b"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenKeyword, Data: "fi"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 85
			"if a && b || c & then d; fi",
			[]parser.Token{
				{Type: TokenKeyword, Data: "if"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "&&"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "||"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "c"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "&"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "then"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "d"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "fi"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 86
			"if a; then b; else c; fi",
			[]parser.Token{
				{Type: TokenKeyword, Data: "if"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "then"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "else"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "c"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "fi"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 87
			"if a; then b; elif c; then d; else if e; then f; fi; fi",
			[]parser.Token{
				{Type: TokenKeyword, Data: "if"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "then"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "elif"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "c"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "then"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "d"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "else"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "if"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "e"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "then"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "f"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "fi"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "fi"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 88
			"while a; do b; done",
			[]parser.Token{
				{Type: TokenKeyword, Data: "while"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "do"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "done"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 89
			"until a && b || c & do b; done",
			[]parser.Token{
				{Type: TokenKeyword, Data: "until"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "&&"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "||"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "c"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "&"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "do"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "done"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 90
			"while a; do break; done",
			[]parser.Token{
				{Type: TokenKeyword, Data: "while"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "do"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "break"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "done"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 91
			"until a; do continue; done",
			[]parser.Token{
				{Type: TokenKeyword, Data: "until"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "do"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "continue"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "done"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 92
			"break",
			[]parser.Token{
				{Type: parser.TokenError, Data: "invalid keyword"},
			},
		},
		{ // 93
			"continue",
			[]parser.Token{
				{Type: parser.TokenError, Data: "invalid keyword"},
			},
		},
		{ // 94
			"for a; do b; done",
			[]parser.Token{
				{Type: TokenKeyword, Data: "for"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "do"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "done"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 95
			"for a do b; done",
			[]parser.Token{
				{Type: TokenKeyword, Data: "for"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "do"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "done"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 96
			"for a\ndo b; done",
			[]parser.Token{
				{Type: TokenKeyword, Data: "for"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenKeyword, Data: "do"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "done"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 97
			"for a in 1 2 3; do b; done",
			[]parser.Token{
				{Type: TokenKeyword, Data: "for"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "in"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "1"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "2"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "3"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "do"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "done"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 98
			"for % in 1 2 3; do b; done",
			[]parser.Token{
				{Type: TokenKeyword, Data: "for"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid identifier"},
			},
		},
		{ // 99
			"for a in 1 2 3 do b; done",
			[]parser.Token{
				{Type: TokenKeyword, Data: "for"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "in"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "1"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "2"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "3"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "do"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "missing do"},
			},
		},
		{ // 100
			"for ((a=1;a<2;a++)) do b; done",
			[]parser.Token{
				{Type: TokenKeyword, Data: "for"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "(("},
				{Type: TokenIdentifierAssign, Data: "a"},
				{Type: TokenPunctuator, Data: "="},
				{Type: TokenNumberLiteral, Data: "1"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWord, Data: "a"},
				{Type: TokenPunctuator, Data: "<"},
				{Type: TokenNumberLiteral, Data: "2"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWord, Data: "a"},
				{Type: TokenPunctuator, Data: "++"},
				{Type: TokenPunctuator, Data: "))"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "do"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "done"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 101
			"for ((a=1;a<2;a++)); do b; done",
			[]parser.Token{
				{Type: TokenKeyword, Data: "for"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "(("},
				{Type: TokenIdentifierAssign, Data: "a"},
				{Type: TokenPunctuator, Data: "="},
				{Type: TokenNumberLiteral, Data: "1"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWord, Data: "a"},
				{Type: TokenPunctuator, Data: "<"},
				{Type: TokenNumberLiteral, Data: "2"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWord, Data: "a"},
				{Type: TokenPunctuator, Data: "++"},
				{Type: TokenPunctuator, Data: "))"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "do"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "done"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 102
			"for ( a=1;a<2;a++ ); do b; done",
			[]parser.Token{
				{Type: TokenKeyword, Data: "for"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 103
			"select a; do b; done",
			[]parser.Token{
				{Type: TokenKeyword, Data: "select"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "do"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "done"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 104
			"select a do b; done",
			[]parser.Token{
				{Type: TokenKeyword, Data: "select"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "do"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "done"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 105
			"select a\ndo b; done",
			[]parser.Token{
				{Type: TokenKeyword, Data: "select"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenKeyword, Data: "do"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "done"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 106
			"select a in 1 2 3; do b; done",
			[]parser.Token{
				{Type: TokenKeyword, Data: "select"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "in"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "1"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "2"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "3"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "do"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "done"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 107
			"select a in 1 2 3 do b; done",
			[]parser.Token{
				{Type: TokenKeyword, Data: "select"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "in"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "1"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "2"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "3"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "do"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "missing do"},
			},
		},
		{ // 108

			"coproc a b",
			[]parser.Token{
				{Type: TokenKeyword, Data: "coproc"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 109

			"coproc fora b",
			[]parser.Token{
				{Type: TokenKeyword, Data: "coproc"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "fora"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 110
			"coproc while a; do b; done",
			[]parser.Token{
				{Type: TokenKeyword, Data: "coproc"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "while"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "do"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "done"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 111
			"coproc a while b; do c; done",
			[]parser.Token{
				{Type: TokenKeyword, Data: "coproc"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "while"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "do"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "c"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "done"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 112
			"echo }",
			[]parser.Token{
				{Type: TokenWord, Data: "echo"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "}"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 113
			"{ echo }",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "{"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "echo"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "}"},
				{Type: parser.TokenError, Data: "unexpected EOF"},
			},
		},
		{ // 114
			"{ echo };}",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "{"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "echo"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "}"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 115
			"function a{ b; }",
			[]parser.Token{
				{Type: TokenKeyword, Data: "function"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenFunctionIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "{"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "}"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 116
			"function a\n{ b; }",
			[]parser.Token{
				{Type: TokenKeyword, Data: "function"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenFunctionIdentifier, Data: "a"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenPunctuator, Data: "{"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "}"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 117
			"function a(){ b; }",
			[]parser.Token{
				{Type: TokenKeyword, Data: "function"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenFunctionIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "("},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenPunctuator, Data: "{"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "}"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 118
			"function a ( ) { b; }",
			[]parser.Token{
				{Type: TokenKeyword, Data: "function"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenFunctionIdentifier, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "("},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "{"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "}"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 119
			"function a() b",
			[]parser.Token{
				{Type: TokenKeyword, Data: "function"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenFunctionIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "("},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid keyword"},
			},
		},
		{ // 120
			"a(){ b; }",
			[]parser.Token{
				{Type: TokenFunctionIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "("},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenPunctuator, Data: "{"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "}"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 121
			"a( ) { b; }",
			[]parser.Token{
				{Type: TokenFunctionIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "("},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "{"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ";"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "}"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 122
			"a() b",
			[]parser.Token{
				{Type: TokenFunctionIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "("},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid keyword"},
			},
		},
		{ // 123
			"a() b",
			[]parser.Token{
				{Type: TokenFunctionIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "("},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid keyword"},
			},
		},
		{ // 124
			"[[ -f file ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "-f"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "file"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "]]"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 125
			"[[ ! -e file\"str\" ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "!"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "-e"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "file"},
				{Type: TokenString, Data: "\"str\""},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "]]"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 126
			"[[ -S \"str\"a || -g $b\"c\"d ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "-S"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenString, Data: "\"str\""},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "||"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "-g"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifier, Data: "$b"},
				{Type: TokenString, Data: "\"c\""},
				{Type: TokenWord, Data: "d"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "]]"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 127
			"[[ a = b ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenOperator, Data: "="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPattern, Data: "b"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "]]"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 128
			"[[ a$b = c\"d\" && e\"f\"g != \"h\"$i ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenIdentifier, Data: "$b"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenOperator, Data: "="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPattern, Data: "c"},
				{Type: TokenString, Data: "\"d\""},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "&&"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "e"},
				{Type: TokenString, Data: "\"f\""},
				{Type: TokenWord, Data: "g"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenOperator, Data: "!="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenString, Data: "\"h\""},
				{Type: TokenIdentifier, Data: "$i"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "]]"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 129
			"[[ a -gt b ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "-gt"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "]]"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 130
			"[[ a$b -eq c\"d\" && e\"f\"g -ne \"h\"$i ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenIdentifier, Data: "$b"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "-eq"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "c"},
				{Type: TokenString, Data: "\"d\""},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "&&"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "e"},
				{Type: TokenString, Data: "\"f\""},
				{Type: TokenWord, Data: "g"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "-ne"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenString, Data: "\"h\""},
				{Type: TokenIdentifier, Data: "$i"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "]]"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 131
			"[[ (a = b || c = d) && $e -le $f ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "("},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenOperator, Data: "="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPattern, Data: "b"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "||"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "c"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenOperator, Data: "="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPattern, Data: "d"},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "&&"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifier, Data: "$e"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "-le"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenIdentifier, Data: "$f"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "]]"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 132
			"[[ (a=b) ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "("},
				{Type: TokenWord, Data: "a"},
				{Type: TokenPunctuator, Data: "="},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "]]"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 133
			"[[ (a = b) ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "("},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenOperator, Data: "="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPattern, Data: "b"},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "]]"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 134
			"[[ (a -gt b) ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "("},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "-gt"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "]]"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 135
			"[[\na\n=\nb\n]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenWord, Data: "a"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenOperator, Data: "="},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenPattern, Data: "b"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenKeyword, Data: "]]"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 136
			"[[\n(a=b)\n]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenPunctuator, Data: "("},
				{Type: TokenWord, Data: "a"},
				{Type: TokenPunctuator, Data: "="},
				{Type: TokenWord, Data: "b"},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenKeyword, Data: "]]"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 137
			"[[ ",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "unexpected EOF"},
			},
		},
		{ // 138
			"[[ | = b ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 139
			"[[ & = b ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 140
			"[[ \"a\" = b ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenString, Data: "\"a\""},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenOperator, Data: "="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPattern, Data: "b"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "]]"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 141
			"[[ ]]a = ]]b ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "]]a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenOperator, Data: "="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPattern, Data: "]]b"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "]]"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 142
			"[[ ( a = ]]b ) ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "("},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenOperator, Data: "="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPattern, Data: "]]b"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: ")"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "]]"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 143
			"[[ ) = ) ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 144
			"[[ ( ]] ) ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "("},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 145
			"[[ a \n= b ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenOperator, Data: "="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPattern, Data: "b"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "]]"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 146
			"[[ a\n = b ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenOperator, Data: "="},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPattern, Data: "b"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenKeyword, Data: "]]"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 147
			"[[ a ",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "unexpected EOF"},
			},
		},
		{ // 148
			"[[ a ! b ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 149
			"[[ a -ez b ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 150
			"[[ a -nz b ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 151
			"[[ a -gz b ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 152
			"[[ a -lz b ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 153
			"[[ a -oz b ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 154
			"[[ a -z b ]]",
			[]parser.Token{
				{Type: TokenKeyword, Data: "[["},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 155
			"\"",
			[]parser.Token{
				{Type: parser.TokenError, Data: "unexpected EOF"},
			},
		},
		{ // 156
			"$((",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "$(("},
				{Type: parser.TokenError, Data: "unexpected EOF"},
			},
		},
		{ // 157
			"$(( \"1\" ))",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "$(("},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenString, Data: "\"1\""},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "))"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 158
			"$(( : ))",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "$(("},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 159
			"$(( ; ))",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "$(("},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 160
			"`\\",
			[]parser.Token{
				{Type: TokenOpenBacktick, Data: "`"},
				{Type: parser.TokenError, Data: "incorrect backtick depth"},
			},
		},
		{ // 161
			"<<",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: parser.TokenError, Data: "unexpected EOF"},
			},
		},
		{ // 162
			"<<a",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: parser.TokenError, Data: "unexpected EOF"},
			},
		},
		{ // 163
			"<<a\\n\\tc\n123\na\n\tc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "a\\n\\tc"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredoc, Data: "123\n"},
				{Type: TokenHeredocEnd, Data: "a\n\tc"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 164
			"<<abc\n123",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "abc"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: parser.TokenError, Data: "unexpected EOF"},
			},
		},
		{ // 165
			"<<abc\n123$\nabc",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "<<"},
				{Type: TokenWord, Data: "abc"},
				{Type: TokenLineTerminator, Data: "\n"},
				{Type: TokenHeredoc, Data: "123$\n"},
				{Type: TokenHeredocEnd, Data: "abc"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 166
			"${a!}",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: parser.TokenError, Data: "invalid parameter expansion"},
			},
		},
		{ // 167
			"${a:b}",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: ":"},
				{Type: parser.TokenError, Data: "invalid parameter expansion"},
			},
		},
		{ // 168
			"${a:1:b}",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: ":"},
				{Type: TokenNumberLiteral, Data: "1"},
				{Type: TokenPunctuator, Data: ":"},
				{Type: parser.TokenError, Data: "invalid parameter expansion"},
			},
		},
		{ // 169
			"${a/(}",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "/"},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 170
			"${a/",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "/"},
				{Type: parser.TokenError, Data: "unexpected EOF"},
			},
		},
		{ // 171
			"${a/)}",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "/"},
				{Type: parser.TokenError, Data: "invalid character"},
			},
		},
		{ // 172
			"${a/b[\\t]+/c}",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "${"},
				{Type: TokenIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "/"},
				{Type: TokenPattern, Data: "b[\\t]+"},
				{Type: TokenPunctuator, Data: "/"},
				{Type: TokenWord, Data: "c"},
				{Type: TokenPunctuator, Data: "}"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 173
			"$(( 0x\"1\" ))",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "$(("},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenWord, Data: "0x"},
				{Type: TokenString, Data: "\"1\""},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenPunctuator, Data: "))"},
				{Type: parser.TokenDone, Data: ""},
			},
		},
		{ // 174
			"$(( 2#, ))",
			[]parser.Token{
				{Type: TokenPunctuator, Data: "$(("},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid number"},
			},
		},
		{ // 175
			"function a time",
			[]parser.Token{
				{Type: TokenKeyword, Data: "function"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenFunctionIdentifier, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid keyword"},
			},
		},
		{ // 176
			"then",
			[]parser.Token{
				{Type: parser.TokenError, Data: "invalid keyword"},
			},
		},
		{ // 177
			"in",
			[]parser.Token{
				{Type: parser.TokenError, Data: "invalid keyword"},
			},
		},
		{ // 178
			"do",
			[]parser.Token{
				{Type: parser.TokenError, Data: "invalid keyword"},
			},
		},
		{ // 179
			"elif",
			[]parser.Token{
				{Type: parser.TokenError, Data: "invalid keyword"},
			},
		},
		{ // 180
			"else",
			[]parser.Token{
				{Type: parser.TokenError, Data: "invalid keyword"},
			},
		},
		{ // 181
			"fi",
			[]parser.Token{
				{Type: parser.TokenError, Data: "invalid keyword"},
			},
		},
		{ // 182
			"done",
			[]parser.Token{
				{Type: parser.TokenError, Data: "invalid keyword"},
			},
		},
		{ // 183
			"esac",
			[]parser.Token{
				{Type: parser.TokenError, Data: "invalid keyword"},
			},
		},
		{ // 184
			"function a coproc",
			[]parser.Token{
				{Type: TokenKeyword, Data: "function"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenFunctionIdentifier, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid keyword"},
			},
		},
		{ // 185
			"function a function",
			[]parser.Token{
				{Type: TokenKeyword, Data: "function"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenFunctionIdentifier, Data: "a"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid keyword"},
			},
		},
		{ // 186
			"function a(\n) { echo b; }",
			[]parser.Token{
				{Type: TokenKeyword, Data: "function"},
				{Type: TokenWhitespace, Data: " "},
				{Type: TokenFunctionIdentifier, Data: "a"},
				{Type: TokenPunctuator, Data: "("},
				{Type: parser.TokenError, Data: "missing closing paren"},
			},
		},
		{ // 187
			"select %; do b; done",
			[]parser.Token{
				{Type: TokenKeyword, Data: "select"},
				{Type: TokenWhitespace, Data: " "},
				{Type: parser.TokenError, Data: "invalid identifier"},
			},
		},
	} {
		p := parser.NewStringTokeniser(test.Input)

		SetTokeniser(&p)

		for m, tkn := range test.Output {
			if tk, _ := p.GetToken(); tk.Type != tkn.Type {
				if tk.Type == parser.TokenError {
					t.Errorf("test %d.%d: unexpected error: %s", n+1, m+1, tk.Data)
				} else {
					t.Errorf("test %d.%d: Incorrect type, expecting %d, got %d", n+1, m+1, tkn.Type, tk.Type)
				}

				break
			} else if tk.Data != tkn.Data {
				t.Errorf("test %d.%d: Incorrect data, expecting %q, got %q", n+1, m+1, tkn.Data, tk.Data)

				break
			}
		}
	}
}
